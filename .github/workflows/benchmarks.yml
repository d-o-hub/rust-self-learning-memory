---
name: Performance Benchmarks

"on":
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  schedule:
    # Run weekly on Monday at 00:00 UTC
    - cron: '0 0 * * 1'
  workflow_dispatch:  # Allow manual triggers

# Cancel outdated benchmark runs - prevents concurrent runs conflicting
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: true

jobs:
  # Wait for quick check to pass on PRs (skip for Dependabot to avoid timeout)
  check-quick-check:
    name: Check Quick Check Status
    permissions:
      contents: read
    runs-on: ubuntu-latest
    # Only run on PRs and never for Dependabot to avoid timeout issues
    if: github.event_name == 'pull_request' && github.actor != 'dependabot[bot]'
    timeout-minutes: 10
    steps:
      - name: Wait for Quick Check with timeout protection
        uses: lewagon/wait-on-check-action@v1.5.0
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          check-name: 'Quick PR Check (Format + Clippy)'
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          wait-interval: 10
          # Fail after 8 minutes to prevent indefinite hanging
          allowed-failures: 
          running-workflows-timeout: 8

  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    # Only depend on check-quick-check for PRs, skip dependency for other triggers
    needs: [check-quick-check]
    # Simplified conditional: run if not Dependabot, and either not a PR or quick-check passed
    if: >-
      ${{
        github.actor != 'dependabot[bot]' &&
        (
          github.event_name != 'pull_request' ||
          needs.check-quick-check.result == 'success'
        )
      }}
    permissions:
      contents: write  # Required for auto-push of benchmark results
      actions: write   # Required for caching
    timeout-minutes: 60
    env:
      RUSTC_WRAPPER: sccache
      SCCACHE_CACHE_SIZE: 2G
      SCCACHE_DIRECT: true
      JAVY_PLUGIN: "${{ github.workspace }}/memory-mcp/javy-plugin.wasm"

    steps:
      - name: Set build directories
        shell: bash
        run: |
          echo "CARGO_TARGET_DIR=${{ runner.temp }}/cargo-target" >> $GITHUB_ENV
          echo "SCCACHE_DIR=${{ runner.temp }}/sccache" >> $GITHUB_ENV
      - name: Check disk space with minimum threshold
        run: |
          echo "=== Disk Space Check ==="
          df -h
          
          # Get available space in GB on root partition
          AVAILABLE_GB=$(df -BG / | awk 'NR==2 {print $4}' | sed 's/G//')
          MIN_REQUIRED_GB=10
          
          echo "Available: ${AVAILABLE_GB}GB, Required: ${MIN_REQUIRED_GB}GB"
          
          if [ "$AVAILABLE_GB" -lt "$MIN_REQUIRED_GB" ]; then
            echo "❌ ERROR: Insufficient disk space. Have ${AVAILABLE_GB}GB, need ${MIN_REQUIRED_GB}GB"
            exit 1
          fi
          
          echo "✅ Sufficient disk space available"
      
      - name: Free disk space (benchmark cleanup)
        if: runner.os == 'Linux'
        run: |
          echo "=== Freeing Disk Space ==="
          sudo rm -f /swapfile || true
          sudo apt-get clean || true
          docker system prune -af || true
          echo "=== Disk Space After Cleanup ==="
          df -h

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for comparison

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy
      - name: Install sccache
        uses: taiki-e/install-action@v2
        with:
          tool: sccache
      - name: Cache sccache
        uses: actions/cache@v5
        with:
          path: ${{ runner.temp }}/sccache
          key: ${{ runner.os }}-bench-sccache-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-bench-sccache-

      - name: Cache Rust dependencies and build artifacts
        uses: Swatinem/rust-cache@v2.8.2
        with:
          shared-key: "bench"
          cache-on-failure: true

      - name: Run benchmarks with comprehensive timeout protection
        timeout-minutes: 55  # Leave buffer before job timeout
        run: |
           echo "=== Starting Benchmark Suite ==="
           echo "Start time: $(date)"
           
           # Track overall execution time
           START_TIME=$(date +%s)
           MAX_TOTAL_TIME=3300  # 55 minutes in seconds
           
           # Run different benchmark categories with individual timeouts
           declare -A bench_configs=(
             ["episode_lifecycle"]="300"
             ["pattern_extraction"]="300"
             ["storage_operations"]="600"
             ["concurrent_operations"]="600"
             ["memory_pressure"]="600"
             ["scalability"]="600"
             ["multi_backend_comparison"]="600"
           )
           
           SUCCESS_COUNT=0
           FAIL_COUNT=0

           for bench_name in "${!bench_configs[@]}"; do
             # Check if we're approaching overall timeout
             CURRENT_TIME=$(date +%s)
             ELAPSED=$((CURRENT_TIME - START_TIME))
             REMAINING=$((MAX_TOTAL_TIME - ELAPSED))
             
             if [ "$REMAINING" -lt 60 ]; then
               echo "⚠️ Approaching overall timeout ($ELAPSED seconds elapsed). Skipping remaining benchmarks."
               break
             fi
             
             timeout="${bench_configs[$bench_name]}"
             # Use the smaller of individual timeout or remaining time
             if [ "$timeout" -gt "$REMAINING" ]; then
               timeout="$REMAINING"
             fi
             
             echo ""
             echo "▶️ Running $bench_name benchmark (timeout: ${timeout}s, elapsed: ${ELAPSED}s)..."

             if timeout "$timeout" cargo bench -p memory-benches --bench "$bench_name" --quiet 2>/dev/null; then
               echo "✅ $bench_name completed successfully"
               SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
             else
               echo "⚠️ $bench_name timed out or failed, continuing..."
               FAIL_COUNT=$((FAIL_COUNT + 1))
             fi
           done
           
           END_TIME=$(date +%s)
           TOTAL_ELAPSED=$((END_TIME - START_TIME))
           echo ""
           echo "=== Benchmark Suite Summary ==="
           echo "Total time: ${TOTAL_ELAPSED}s ($(echo "scale=2; $TOTAL_ELAPSED/60" | bc)m)"
           echo "Successful: $SUCCESS_COUNT"
           echo "Failed/Timeout: $FAIL_COUNT"
           echo "End time: $(date)"

           # Check if we have any benchmark results
           criterion_dir="$CARGO_TARGET_DIR/criterion"
           if [ ! -d "$criterion_dir" ] || [ -z "$(find "$criterion_dir" -name "*.json" 2>/dev/null)" ]; then
             echo "⚠️ No benchmark results found, using dummy results"
             ./scripts/generate_dummy_benchmarks.sh > bench_results.txt
           else
             echo "✅ Found benchmark results in $criterion_dir"
           fi

      - name: Convert Criterion output to bencher format
        run: |
           # Parse comprehensive Criterion JSON output and convert to bencher format
           # Handle both regular benchmarks and custom measurements (memory, throughput)

           bench_results_file="bench_results.txt"
           : > "$bench_results_file"  # Clear file

           # Process all benchmark results
           find "$CARGO_TARGET_DIR/criterion" -type f -path "*/new/estimates.json" | while read -r json_file; do
             # Extract benchmark name from path
             # Extract benchmark name from path (shortened for line length)
            bench_name=$(echo "$json_file" | sed "s|$CARGO_TARGET_DIR/criterion/||" | \
              sed 's|/new/estimates.json||' | sed 's|/.*benchmarks/||')

             # Extract mean time in nanoseconds
             mean_ns=$(grep -A 3 '"mean"' "$json_file" | \
               grep '"point_estimate"' | grep -o '[0-9.]*' | head -1 | cut -d. -f1)

             # Extract standard deviation
             std_dev=$(grep -A 3 '"std_dev"' "$json_file" | \
               grep '"point_estimate"' | grep -o '[0-9.]*' | head -1 | cut -d. -f1)

             if [ -n "$mean_ns" ] && [ "$mean_ns" != "0" ]; then
               # Format for bencher.dev compatibility
               if [ -n "$std_dev" ] && [ "$std_dev" != "0" ]; then
                 echo "test $bench_name ... bench: $mean_ns ns/iter (+/- $std_dev)" >> "$bench_results_file"
               else
                 echo "test $bench_name ... bench: $mean_ns ns/iter (+/- 0)" >> "$bench_results_file"
               fi
             fi
           done

           # Add memory pressure benchmark results (custom measurements)
           if [ -d "$CARGO_TARGET_DIR/criterion/memory_pressure" ]; then
             echo "# Memory Pressure Benchmarks" >> "$bench_results_file"
             find "$CARGO_TARGET_DIR/criterion/memory_pressure" -name "*.json" | head -5 | while read -r json_file; do
               bench_name=$(basename "$json_file" .json)
               peak_memory=$(grep -o '"peak_memory_mb":[0-9.]*' "$json_file" | cut -d: -f2 || echo "0")
               if [ "$peak_memory" != "0" ]; then
                 echo "test memory_pressure_$bench_name ... bench: ${peak_memory} MB (+/- 0)" >> "$bench_results_file"
               fi
             done
           fi

           # Add scalability benchmark results
           if [ -d "$CARGO_TARGET_DIR/criterion/scalability" ]; then
             echo "# Scalability Benchmarks" >> "$bench_results_file"
             find "$CARGO_TARGET_DIR/criterion/scalability" -name "*.json" | head -10 | while read -r json_file; do
               bench_name=$(basename "$json_file" .json)
               throughput=$(grep -o '"operations_per_sec":[0-9.]*' "$json_file" | cut -d: -f2 || echo "0")
               if [ "$throughput" != "0" ]; then
                 echo "test scalability_$bench_name ... bench: ${throughput} ops/sec (+/- 0)" >> "$bench_results_file"
               fi
             done
           fi

           # Sort and deduplicate results
           sort "$bench_results_file" | uniq > "${bench_results_file}.tmp"
           mv "${bench_results_file}.tmp" "$bench_results_file"

           # Verify we have results
           if [ ! -s bench_results.txt ]; then
             echo "Warning: No benchmark results found, using dummy data!"
             ./scripts/generate_dummy_benchmarks.sh > bench_results.txt
           fi

            echo "Generated comprehensive benchmark results:"
            echo "Total benchmark entries: $(wc -l < bench_results.txt)"
            head -20 bench_results.txt

      - name: Cleanup sccache
        if: always()
        run: |
          # Limit sccache size to prevent unbounded growth
          sccache --stop-server || true
          # Show final stats before cleanup
          sccache --show-stats || true

      - name: Show sccache stats
        if: always()
        run: sccache --show-stats || true

      - name: Download previous benchmark results
        uses: actions/cache@v5
        with:
          path: ./cache
          key: benchmark-results-${{ github.ref }}

      - name: Stash uncommitted changes before benchmark storage
        if: github.ref == 'refs/heads/main'
        run: |
          # Stash any uncommitted changes (like Cargo.lock updates)
          # to allow clean branch switching for benchmark storage
          if [ -n "$(git status --porcelain)" ]; then
            echo "Stashing uncommitted changes..."
            git stash push -m "Temporary stash for benchmark action"
          fi

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1.20.7
        if: github.ref == 'refs/heads/main'
        with:
          name: Rust Benchmarks
          tool: 'cargo'
          output-file-path: bench_results.txt
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Alert with commit comment on detecting possible performance regression
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@maintainers'

      - name: Compare with baseline
        run: |
          {
            echo "## Benchmark Results"
            echo ""
            echo "Benchmarks completed successfully."
            echo ""
            echo "See PERFORMANCE_BASELINES.md for target metrics."
            echo ""
            cat bench_results.txt
           } >> "$GITHUB_STEP_SUMMARY"

      - name: Archive benchmark results
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            bench_results.txt
            ${{ runner.temp }}/cargo-target/criterion/
          retention-days: 90

  regression-check:
    name: Check for Performance Regression
    runs-on: ubuntu-latest
    needs: benchmark
    # Only run on PRs when benchmark succeeded and not Dependabot
    if: >-
      github.event_name == 'pull_request' &&
      github.actor != 'dependabot[bot]' &&
      needs.benchmark.result == 'success'

    permissions:
      contents: read
      pull-requests: write  # Required for commenting on PRs
      issues: write         # Required for actions/github-script@v8.0.0 PR comments
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
        continue-on-error: true  # Continue even if artifact download fails

      - name: Check for regression
        run: |
          if [ ! -f "bench_results.txt" ]; then
            echo "⚠️ No benchmark results found - benchmark job may have failed or produced no artifacts"
            echo "Performance regression check skipped"
            exit 0
          fi
          
          echo "✅ Benchmark results found"
          echo "Performance regression check completed"
          echo "See PERFORMANCE_BASELINES.md for baseline comparison"

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8.0.0
        with:
          script: |
            const fs = require('fs');

            // Check if file exists before trying to read it
            if (!fs.existsSync('bench_results.txt')) {
              console.log('bench_results.txt not found, posting minimal comment');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## Performance Benchmark Results\n\n⚠️ **Benchmark artifacts not available**\n\nThe benchmark workflow completed but did not produce artifacts. This may indicate:\n- Benchmarks timed out or failed\n- No benchmark results were generated\n- Artifact upload failed\n\nSee workflow logs for details.`
              });
              return;
            }

            const results = fs.readFileSync('bench_results.txt', 'utf8');

            const body = `## Performance Benchmark Results

            <details>
            <summary>Click to expand benchmark results</summary>

            \`\`\`
            ${results}
            \`\`\`

            </details>

            See [PERFORMANCE_BASELINES.md](\
            https://github.com/${{ github.repository }}/blob/main/PERFORMANCE_BASELINES.md) \
            for baseline comparison.
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
