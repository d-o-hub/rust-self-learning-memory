name: Performance Benchmarks

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  schedule:
    # Run weekly on Monday at 00:00 UTC
    - cron: '0 0 * * 1'
  workflow_dispatch:  # Allow manual triggers

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for comparison

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: ~/.cargo/registry
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}

      - name: Cache cargo index
        uses: actions/cache@v4
        with:
          path: ~/.cargo/git
          key: ${{ runner.os }}-cargo-git-${{ hashFiles('**/Cargo.lock') }}

      - name: Cache cargo build
        uses: actions/cache@v4
        with:
          path: target
          key: ${{ runner.os }}-cargo-build-bench-${{ hashFiles('**/Cargo.lock') }}

      - name: Run benchmarks
        run: |
          # Run benchmarks - Criterion will generate JSON output in target/criterion
          cargo bench --package memory-benches --bench episode_lifecycle
          cargo bench --package memory-benches --bench pattern_extraction
          cargo bench --package memory-benches --bench storage_operations

      - name: Convert Criterion output to bencher format
        run: |
          # Parse Criterion JSON output and convert to bencher format
          # Only process "new" estimates to avoid duplicates
          find target/criterion -type f -path "*/new/estimates.json" | while read -r json_file; do
            # Extract benchmark name from path
            # Path format: target/criterion/<benchmark_name>/new/estimates.json
            bench_name=$(echo "$json_file" | sed 's|target/criterion/||' | sed 's|/new/estimates.json||' | sed 's|/|-|g')

            # Extract mean time in nanoseconds from JSON
            mean_ns=$(grep -o '"point_estimate":[0-9.]*' "$json_file" | head -1 | cut -d: -f2 | cut -d. -f1)

            if [ -n "$mean_ns" ] && [ "$mean_ns" != "0" ]; then
              echo "test $bench_name ... bench: $mean_ns ns/iter (+/- 0)"
            fi
          done > bench_results.txt

          # Verify we have results
          if [ ! -s bench_results.txt ]; then
            echo "Error: No benchmark results found!"
            echo "Listing criterion output:"
            find target/criterion -type f -name "*.json" | head -20
            exit 1
          fi

          echo "Generated benchmark results:"
          cat bench_results.txt

      - name: Download previous benchmark results
        uses: actions/cache@v4
        with:
          path: ./cache
          key: benchmark-results-${{ github.ref }}

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        if: github.ref == 'refs/heads/main'
        with:
          name: Rust Benchmarks
          tool: 'cargo'
          output-file-path: bench_results.txt
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Alert with commit comment on detecting possible performance regression
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@maintainers'

      - name: Compare with baseline
        run: |
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmarks completed successfully." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "See PERFORMANCE_BASELINES.md for target metrics." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat bench_results.txt >> $GITHUB_STEP_SUMMARY

      - name: Archive benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            bench_results.txt
            target/criterion/
          retention-days: 90

  regression-check:
    name: Check for Performance Regression
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'

    permissions:
      contents: read
      pull-requests: write  # Required for commenting on PRs

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}

      - name: Check for regression
        run: |
          # This is a placeholder for custom regression detection
          # You can add a script here to compare against baselines
          echo "Performance regression check completed"
          echo "See PERFORMANCE_BASELINES.md for baseline comparison"

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = fs.readFileSync('bench_results.txt', 'utf8');
            
            const body = `## Performance Benchmark Results
            
            <details>
            <summary>Click to expand benchmark results</summary>
            
            \`\`\`
            ${results}
            \`\`\`
            
            </details>
            
            See [PERFORMANCE_BASELINES.md](https://github.com/${{ github.repository }}/blob/main/PERFORMANCE_BASELINES.md) for baseline comparison.
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
