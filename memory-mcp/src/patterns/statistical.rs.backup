//! # Statistical Analysis Engine
//!
//! Core statistical engine providing Bayesian changepoint detection,
//! correlation analysis with significance testing, and time-series trend detection.

use anyhow::{anyhow, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::collections::VecDeque;
use tracing::{debug, info, instrument, warn};

/// BOCD (Bayesian Online Change Point Detection) state
#[derive(Debug, Clone)]
pub struct BOCPDState {
    /// Posterior probabilities over run lengths (in log space)
    pub log_posterior: Vec<f64>,
    /// Prior probabilities for run lengths
    pub log_prior: Vec<f64>,
    /// Current hazard rate
    pub hazard_rate: f64,
    /// Expected run length for truncation
    pub expected_run_length: f64,
    /// Circular buffer for data
    pub data_buffer: VecDeque<f64>,
    /// Maximum buffer size
    pub max_buffer_size: usize,
    /// Number of processed points
    pub processed_points: usize,
}

/// Configuration for BOCD detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BOCPDConfig {
    /// Hazard rate for change point detection
    pub hazard_rate: f64,
    /// Expected run length (for truncation)
    pub expected_run_length: usize,
    /// Maximum number of run length hypotheses
    pub max_run_length_hypotheses: usize,
    /// Alert threshold for change point confidence
    pub alert_threshold: f64,
    /// Enable multi-resolution detection
    pub multi_resolution: bool,
    /// Short-term analysis window
    pub short_window: usize,
    /// Medium-term analysis window
    pub medium_window: usize,
    /// Long-term analysis window
    pub long_window: usize,
}

impl Default for BOCPDConfig {
    fn default() -> Self {
        Self {
            hazard_rate: 250.0,
            expected_run_length: 250,
            max_run_length_hypotheses: 1000,
            alert_threshold: 0.7,
            multi_resolution: true,
            short_window: 50,
            medium_window: 200,
            long_window: 500,
        }
    }
}

/// BOCPD detection result with uncertainty quantification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BOCPDResult {
    /// Detected changepoint index
    pub changepoint_index: Option<usize>,
    /// Posterior probability of changepoint
    pub changepoint_probability: f64,
    /// Maximum a posteriori run length
    pub map_run_length: usize,
    /// Posterior distribution over run lengths
    pub run_length_distribution: Vec<f64>,
    /// Collective anomaly indicator
    pub collective_anomaly: bool,
    /// Detection confidence
    pub confidence: f64,
    /// Time resolution level
    pub resolution_level: ResolutionLevel,
}

/// Resolution levels for multi-scale analysis
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum ResolutionLevel {
    Short,
    Medium,
    Long,
}

/// Streaming BOCPD detector
#[derive(Debug)]
pub struct StreamingBOCPD {
    config: BOCPDConfig,
    short_term_state: BOCPDState,
    medium_term_state: BOCPDState,
    long_term_state: BOCPDState,
}

/// Bayesian Online Change Point Detection Functions
impl StreamingBOCPD {
    /// Create a new streaming BOCPD detector
    pub fn new(config: BOCPDConfig) -> Self {
        let short_term_state = BOCPDState {
            log_posterior: vec![0.0; config.max_run_length_hypotheses],
            log_prior: Self::initialize_prior(config.expected_run_length, config.max_run_length_hypotheses),
            hazard_rate: config.hazard_rate,
            expected_run_length: config.expected_run_length as f64,
            data_buffer: VecDeque::with_capacity(config.short_window),
            max_buffer_size: config.short_window,
            processed_points: 0,
        };

        let medium_term_state = BOCPDState {
            log_posterior: vec![0.0; config.max_run_length_hypotheses],
            log_prior: Self::initialize_prior(config.expected_run_length, config.max_run_length_hypotheses),
            hazard_rate: config.hazard_rate * 0.8, // Slightly more conservative for medium term
            expected_run_length: config.expected_run_length as f64,
            data_buffer: VecDeque::with_capacity(config.medium_window),
            max_buffer_size: config.medium_window,
            processed_points: 0,
        };

        let long_term_state = BOCPDState {
            log_posterior: vec![0.0; config.max_run_length_hypotheses],
            log_prior: Self::initialize_prior(config.expected_run_length, config.max_run_length_hypotheses),
            hazard_rate: config.hazard_rate * 0.6, // Most conservative for long term
            expected_run_length: config.expected_run_length as f64,
            data_buffer: VecDeque::with_capacity(config.long_window),
            max_buffer_size: config.long_window,
            processed_points: 0,
        };

        Self {
            config,
            short_term_state,
            medium_term_state,
            long_term_state,
        }
    }

    /// Initialize prior probabilities for run lengths
    fn initialize_prior(expected_run_length: usize, max_hypotheses: usize) -> Vec<f64> {
        let mut log_prior = vec![f64::NEG_INFINITY; max_hypotheses];
        
        // Geometric prior for run lengths
        let p_geometric = 1.0 / (expected_run_length as f64 + 1.0);
        
        for r in 0..max_hypotheses {
            log_prior[r] = if r == 0 {
                p_geometric.ln()
            } else {
                ((1.0 - p_geometric) * p_geometric.powi(r as i32 - 1)).ln()
            }
        }
        
        log_prior
    }

    /// Detect changepoints using unified BOCD framework
    pub fn detect_changepoints(&mut self, data: &[f64]) -> Result<Vec<BOCPDResult>> {
        let mut results = Vec::new();

        // Clone config once so we can mutably borrow internal states without borrowing `self`.
        let config = self.config.clone();

        for &value in data {
            // Update all resolution levels exactly once per observation.
            let short_result = Self::update_single_state(&config, &mut self.short_term_state, value)?;
            let medium_result = Self::update_single_state(&config, &mut self.medium_term_state, value)?;
            let long_result = Self::update_single_state(&config, &mut self.long_term_state, value)?;

            // Extract changepoints if detected
            if let Some((index, prob)) = self.extract_changepoints(&short_result)? {
                results.push(BOCPDResult {
                    changepoint_index: Some(index),
                    changepoint_probability: prob,
                    map_run_length: self.compute_map_run_length(&short_result),
                    run_length_distribution: self.normalize_distribution(&short_result.log_posterior),
                    collective_anomaly: self.detect_collective_anomaly(&short_result),
                    confidence: prob.min(1.0),
                    resolution_level: ResolutionLevel::Short,
                });
            }

            if let Some((index, prob)) = self.extract_changepoints(&medium_result)? {
                results.push(BOCPDResult {
                    changepoint_index: Some(index),
                    changepoint_probability: prob,
                    map_run_length: self.compute_map_run_length(&medium_result),
                    run_length_distribution: self.normalize_distribution(&medium_result.log_posterior),
                    collective_anomaly: self.detect_collective_anomaly(&medium_result),
                    confidence: prob.min(1.0),
                    resolution_level: ResolutionLevel::Medium,
                });
            }

            if let Some((index, prob)) = self.extract_changepoints(&long_result)? {
                results.push(BOCPDResult {
                    changepoint_index: Some(index),
                    changepoint_probability: prob,
                    map_run_length: self.compute_map_run_length(&long_result),
                    run_length_distribution: self.normalize_distribution(&long_result.log_posterior),
                    collective_anomaly: self.detect_collective_anomaly(&long_result),
                    confidence: prob.min(1.0),
                    resolution_level: ResolutionLevel::Long,
                });
            }
        }

        Ok(results)
    }

    /// Update BOCPD state with new observation (recursive posterior update)
    pub fn update_bocpd_state(&mut self, observation: f64) -> Result<BOCPDState> {
        let config = self.config.clone();

        // Update all three states; return the short-term snapshot.
        let short_result = Self::update_single_state(&config, &mut self.short_term_state, observation)?;
        let _medium_result = Self::update_single_state(&config, &mut self.medium_term_state, observation)?;
        let _long_result = Self::update_single_state(&config, &mut self.long_term_state, observation)?;

        Ok(short_result)
    }

    /// Update a single BOCPD state
    fn update_single_state(
        config: &BOCPDConfig,
        state: &mut BOCPDState,
        observation: f64,
    ) -> Result<BOCPDState> {
        // Add to circular buffer
        state.data_buffer.push_back(observation);
        if state.data_buffer.len() > state.max_buffer_size {
            state.data_buffer.pop_front();
        }
        
        state.processed_points += 1;
        
        // Update hazard rates adaptively
        state.hazard_rate = self.compute_hazard_rates(state)?;
        
        // Update posterior distribution
        let new_posterior = self.update_posterior_distribution(state, observation)?;
        
        Ok(BOCPDState {
            log_posterior: new_posterior,
            log_prior: state.log_prior.clone(),
            hazard_rate: state.hazard_rate,
            expected_run_length: state.expected_run_length,
            data_buffer: state.data_buffer.clone(),
            max_buffer_size: state.max_buffer_size,
            processed_points: state.processed_points,
        })
    }

    /// Compute adaptive hazard rates based on recent data patterns
    pub fn compute_hazard_rates(&self, state: &BOCPDState) -> Result<f64> {
        if state.data_buffer.len() < 10 {
            return Ok(state.hazard_rate);
        }

        // Compute recent variance to adapt hazard rate
        let recent_data: Vec<f64> = state
            .data_buffer
            .iter()
            .rev()
            .take(20)
            .copied()
            .collect();
        let variance = self.compute_variance(&recent_data)?;

        // Adaptive hazard rate: higher variance -> higher hazard rate
        let base_hazard = self.config.hazard_rate;
        let variance_factor = (variance / 10.0).min(3.0); // Cap the factor
        let adaptive_hazard = base_hazard * variance_factor;

        // Smooth the hazard rate to avoid rapid changes
        let smoothed_hazard = 0.9 * state.hazard_rate + 0.1 * adaptive_hazard;

        Ok(smoothed_hazard)
    }

    /// Update posterior distribution using BOCPD recurrence relation
    fn update_posterior_distribution(&self, state: &BOCPDState, observation: f64) -> Result<Vec<f64>> {
        let max_r = state.log_posterior.len() - 1;
        let mut new_posterior = vec![f64::NEG_INFINITY; state.log_posterior.len()];
        
        // Calculate hazard probability
        let hazard_prob = (state.hazard_rate / (1.0 + state.hazard_rate)).ln();
        let survival_prob = (1.0 / (1.0 + state.hazard_rate)).ln();
        
        for r in 0..=max_r {
            if r == 0 {
                // Changepoint case: sum over all previous run lengths
                let mut log_sum = f64::NEG_INFINITY;
                
                for prev_r in 0..=max_r {
                    if state.log_posterior[prev_r].is_finite() {
                        let term = state.log_posterior[prev_r] + hazard_prob;
                        log_sum = log_add_exp(log_sum, term);
                    }
                }
                
                new_posterior[0] = log_sum + self.compute_likelihood(state, observation)?;
            } else {
                // Continuity case: extend previous run length
                if state.log_posterior[r - 1].is_finite() {
                    new_posterior[r] = state.log_posterior[r - 1] + survival_prob + self.compute_likelihood(state, observation)?;
                }
            }
        }
        
        // Normalize using log-sum-exp trick
        let log_normalizer = log_sum_exp(&new_posterior);
        for val in &mut new_posterior {
            if val.is_finite() {
                *val -= log_normalizer;
            }
        }
        
        Ok(new_posterior)
    }

    /// Compute likelihood of observation under current state
    fn compute_likelihood(&self, state: &BOCPDState, observation: f64) -> Result<f64> {
        if state.data_buffer.len() < 2 {
            return Ok(0.0); // Uniform prior for insufficient data
        }
        
        // Compute mean and variance from recent data (excluding current observation)
        let recent_data: Vec<f64> = state.data_buffer.iter().rev().skip(1).take(10).cloned().collect();
        
        if recent_data.is_empty() {
            return Ok(0.0);
        }
        
        let mean = recent_data.iter().sum::<f64>() / recent_data.len() as f64;
        let variance = recent_data.iter().map(|&x| (x - mean).powi(2)).sum::<f64>() / recent_data.len() as f64;
        let std_dev = variance.sqrt().max(0.01); // Avoid division by zero
        
        // Gaussian likelihood
        let z_score = (observation - mean) / std_dev;
        let log_likelihood = -0.5 * (z_score.powi(2) + (2.0 * std_dev * std_dev * std_dev).ln());
        
        Ok(log_likelihood)
    }

    /// Extract changepoints using MAP estimation
    pub fn extract_changepoints(&self, state: &BOCPDState) -> Result<Option<(usize, f64)>> {
        if state.processed_points < 10 {
            return Ok(None); // Need minimum data for reliable detection
        }
        
        // Find MAP estimate for changepoint probability
        let changepoint_prob = state.log_posterior[0].exp();
        
        // Detect changepoint if probability exceeds threshold
        if changepoint_prob > self.config.alert_threshold {
            return Ok(Some((state.processed_points - 1, changepoint_prob)));
        }
        
        Ok(None)
    }

    /// Compute Maximum A Posteriori run length
    fn compute_map_run_length(&self, state: &BOCPDState) -> usize {
        let mut max_log_prob = f64::NEG_INFINITY;
        let mut map_index = 0;
        
        for (i, &log_prob) in state.log_posterior.iter().enumerate() {
            if log_prob > max_log_prob {
                max_log_prob = log_prob;
                map_index = i;
            }
        }
        
        map_index
    }

    /// Normalize posterior distribution to probabilities
    fn normalize_distribution(&self, log_posterior: &[f64]) -> Vec<f64> {
        let log_normalizer = log_sum_exp(log_posterior);
        log_posterior.iter().map(|&x| (x - log_normalizer).exp()).collect()
    }

    /// Detect collective anomalies
    fn detect_collective_anomaly(&self, state: &BOCPDState) -> bool {
        if state.data_buffer.len() < 5 {
            return false;
        }
        
        let recent_data: Vec<f64> = state.data_buffer.iter().rev().take(5).cloned().collect();
        let variance = self.compute_variance(&recent_data).unwrap_or(0.0);
        
        // Detect collective anomaly if variance is unusually high
        variance > 100.0
    }

    /// Compute variance of a data series
    fn compute_variance(&self, data: &[f64]) -> Result<f64> {
        if data.len() < 2 {
            return Ok(0.0);
        }
        
        let mean = data.iter().sum::<f64>() / data.len() as f64;
        let variance = data.iter().map(|&x| (x - mean).powi(2)).sum::<f64>() / data.len() as f64;
        
        Ok(variance)
    }
}

/// Utility functions for numerical stability
fn log_add_exp(a: f64, b: f64) -> f64 {
    if a.is_infinite() && a < 0.0 {
        return b;
    }
    if b.is_infinite() && b < 0.0 {
        return a;
    }
    
    let max_val = a.max(b);
    if (a - b).abs() > 50.0 {
        // Prevent numerical overflow
        max_val
    } else {
        max_val + ((a - max_val).exp() + (b - max_val).exp()).ln()
    }
}

/// Compute log-sum-exp of a vector in log space
fn log_sum_exp(values: &[f64]) -> f64 {
    if values.is_empty() {
        return f64::NEG_INFINITY;
    }
    
    let max_val = values
        .iter()
        .fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    
    if max_val.is_infinite() && max_val < 0.0 {
        return max_val;
    }
    
    let sum: f64 = values
        .iter()
        .filter(|&&x| x.is_finite())
        .map(|&x| (x - max_val).exp())
        .sum();
    
    if sum == 0.0 {
        max_val
    } else {
        max_val + sum.ln()
    }
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatisticalConfig {
    /// Significance level for statistical tests (default: 0.05)
    pub significance_level: f64,
    /// Maximum number of data points to analyze (default: 10,000)
    pub max_data_points: usize,
    /// Enable parallel processing (default: true)
    pub parallel_processing: bool,
    /// Bayesian changepoint detection parameters
    pub changepoint_config: ChangepointConfig,
}

impl Default for StatisticalConfig {
    fn default() -> Self {
        Self {
            significance_level: 0.05,
            max_data_points: 10_000,
            parallel_processing: true,
            changepoint_config: ChangepointConfig::default(),
        }
    }
}

/// Configuration for changepoint detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChangepointConfig {
    /// Hazard rate for Bayesian changepoint detection
    pub hazard_rate: f64,
    /// Expected run length for truncated BOCPD
    pub expected_run_length: f64,
}

impl Default for ChangepointConfig {
    fn default() -> Self {
        Self {
            hazard_rate: 250.0,
            expected_run_length: 250.0,
        }
    }
}

/// Statistical analysis results
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatisticalResults {
    /// Correlation coefficients with significance
    pub correlations: Vec<CorrelationResult>,
    /// Detected changepoints
    pub changepoints: Vec<ChangepointResult>,
    /// Trend analysis results
    pub trends: Vec<TrendResult>,
    /// Analysis metadata
    pub metadata: AnalysisMetadata,
}

/// Correlation analysis result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CorrelationResult {
    /// Variable pair
    pub variables: (String, String),
    /// Pearson correlation coefficient
    pub coefficient: f64,
    /// P-value for significance test
    pub p_value: f64,
    /// Whether correlation is statistically significant
    pub significant: bool,
    /// Confidence interval
    pub confidence_interval: (f64, f64),
}

/// Changepoint detection result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChangepointResult {
    /// Index where changepoint was detected
    pub index: usize,
    /// Confidence score (0.0 to 1.0)
    pub confidence: f64,
    /// Type of changepoint detected
    pub change_type: ChangeType,
}

/// Types of changes that can be detected
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ChangeType {
    /// Mean shift
    MeanShift,
    /// Variance change
    VarianceChange,
    /// Trend change
    TrendChange,
    /// Unknown/unspecified
    Unknown,
}

/// Trend analysis result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrendResult {
    /// Variable name
    pub variable: String,
    /// Trend direction
    pub direction: TrendDirection,
    /// Trend strength (0.0 to 1.0)
    pub strength: f64,
    /// Statistical significance
    pub significant: bool,
}

/// Trend directions
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub enum TrendDirection {
    /// Increasing trend
    Increasing,
    /// Decreasing trend
    Decreasing,
    /// No clear trend
    Stationary,
    /// Oscillating pattern
    Oscillating,
}

/// Analysis metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisMetadata {
    /// Number of data points analyzed
    pub data_points: usize,
    /// Analysis duration in milliseconds
    pub duration_ms: u64,
    /// Memory usage in bytes
    pub memory_usage: usize,
    /// Processing method used
    pub processing_method: String,
}

/// Core statistical analysis engine
#[derive(Debug)]
pub struct StatisticalEngine {
    config: StatisticalConfig,
}

impl StatisticalEngine {
    /// Create a new statistical engine with default configuration
    pub fn new() -> Result<Self> {
        Self::with_config(StatisticalConfig::default())
    }

    /// Create a new statistical engine with custom configuration
    pub fn with_config(config: StatisticalConfig) -> Result<Self> {
        Ok(Self { config })
    }

    /// Perform comprehensive statistical analysis on time series data
    #[instrument(skip(self, data), fields(data_points = data.len()))]
    pub fn analyze_time_series(
        &mut self,
        data: &HashMap<String, Vec<f64>>,
    ) -> Result<StatisticalResults> {
        let start_time = std::time::Instant::now();

        info!("Starting statistical analysis of {} variables", data.len());

        // Validate input data
        self.validate_data(data)?;

        // Perform correlation analysis
        let correlations = self.analyze_correlations(data)?;

        // Detect changepoints
        let changepoints = self.detect_changepoints(data)?;

        // Analyze trends
        let trends = self.analyze_trends(data)?;

        // Calculate metadata
        let duration = start_time.elapsed();
        let metadata = AnalysisMetadata {
            data_points: data.values().next().map(|v| v.len()).unwrap_or(0),
            duration_ms: duration.as_millis() as u64,
            memory_usage: self.estimate_memory_usage(data),
            processing_method: if self.config.parallel_processing {
                "parallel".to_string()
            } else {
                "sequential".to_string()
            },
        };

        let results = StatisticalResults {
            correlations,
            changepoints,
            trends,
            metadata,
        };

        info!(
            "Statistical analysis completed in {}ms",
            results.metadata.duration_ms
        );

        Ok(results)
    }

    /// Analyze correlations between variables
    fn analyze_correlations(
        &self,
        data: &HashMap<String, Vec<f64>>,
    ) -> Result<Vec<CorrelationResult>> {
        let mut results = Vec::new();
        let variables: Vec<&String> = data.keys().collect();

        let pairs: Vec<_> = variables
            .iter()
            .enumerate()
            .flat_map(|(i, &var1)| variables[i + 1..].iter().map(move |&var2| (var1, var2)))
            .collect();

        for (var1, var2) in pairs {
            if let (Some(data1), Some(data2)) = (data.get(var1), data.get(var2)) {
                if let Some(corr_result) = self.calculate_correlation(var1, var2, data1, data2)? {
                    results.push(corr_result);
                }
            }
        }

        debug!("Calculated {} correlation pairs", results.len());
        Ok(results)
    }

    /// Calculate correlation between two variables with significance testing
    fn calculate_correlation(
        &self,
        var1: &str,
        var2: &str,
        data1: &[f64],
        data2: &[f64],
    ) -> Result<Option<CorrelationResult>> {
        if data1.len() != data2.len() || data1.len() < 3 {
            return Ok(None);
        }

        // Calculate Pearson correlation coefficient
        let mean1 = data1.iter().sum::<f64>() / data1.len() as f64;
        let mean2 = data2.iter().sum::<f64>() / data2.len() as f64;

        let mut numerator = 0.0;
        let mut sum_sq1 = 0.0;
        let mut sum_sq2 = 0.0;

        for (&x, &y) in data1.iter().zip(data2.iter()) {
            let dx = x - mean1;
            let dy = y - mean2;
            numerator += dx * dy;
            sum_sq1 += dx * dx;
            sum_sq2 += dy * dy;
        }

        let denominator = (sum_sq1 * sum_sq2).sqrt();
        if denominator == 0.0 {
            return Ok(None);
        }

        let coefficient = numerator / denominator;

        // Calculate t-statistic for significance test
        let n = data1.len() as f64;

        // Calculate p-value for correlation
        let p_value = if n < 3.0 {
            1.0 // Not enough data, p-value = 1 (not significant)
        } else {
            let t_stat = coefficient * ((n - 2.0) / (1.0 - coefficient * coefficient)).sqrt();
            2.0 * (1.0 - Self::t_cdf(t_stat.abs(), n - 2.0))
        };

        // For near-perfect correlations or small samples, use simplified significance test
        // Strong correlation (|r| > 0.9) with n >= 3 is considered significant
        let significant = if coefficient.abs() > 0.9 && n >= 3.0 {
            true
        } else {
            p_value < self.config.significance_level
        };

        // Calculate confidence interval (simplified)
        let se = (1.0 - coefficient * coefficient) / (n - 2.0).sqrt();
        let margin = 1.96 * se; // 95% confidence
        let confidence_interval = (
            (coefficient - margin).max(-1.0),
            (coefficient + margin).min(1.0),
        );

        Ok(Some(CorrelationResult {
            variables: (var1.to_string(), var2.to_string()),
            coefficient,
            p_value,
            significant,
            confidence_interval,
        }))
    }

    /// Detect changepoints in time series data
    fn detect_changepoints(
        &mut self,
        data: &HashMap<String, Vec<f64>>,
    ) -> Result<Vec<ChangepointResult>> {
        let mut results = Vec::new();

        for (var_name, series) in data {
            if series.len() < 10 {
                continue;
            }

            // Initialize BOCPD detector with current configuration
            let bocpd_config = BOCPDConfig {
                hazard_rate: self.config.changepoint_config.hazard_rate,
                expected_run_length: self.config.changepoint_config.expected_run_length as usize,
                max_run_length_hypotheses: 1000,
                alert_threshold: 0.7,
                multi_resolution: true,
                short_window: 50.min(series.len() / 4),
                medium_window: 200.min(series.len() / 2),
                long_window: 500.min(series.len()),
            };

            let mut bocpd = StreamingBOCPD::new(bocpd_config);

            // Run BOCPD detection
            let bocpd_results = bocpd.detect_changepoints(series)?;

            // Convert BOCPD results to standard format
            for bocpd_result in bocpd_results {
                if let Some(changepoint_index) = bocpd_result.changepoint_index {
                    // Validate changepoint index is within bounds
                    if changepoint_index < series.len() {
                        let change_type = if bocpd_result.collective_anomaly {
                            ChangeType::VarianceChange
                        } else {
                            ChangeType::MeanShift
                        };

                        results.push(ChangepointResult {
                            index: changepoint_index,
                            confidence: bocpd_result.confidence,
                            change_type,
                        });
                    }
                }
            }

            debug!("Detected {} changepoints in {}", results.len(), var_name);
        }

        Ok(results)
    }

    /// Analyze trends in time series data
    fn analyze_trends(&self, data: &HashMap<String, Vec<f64>>) -> Result<Vec<TrendResult>> {
        let mut results = Vec::new();

        for (var_name, series) in data {
            if series.len() < 5 {
                continue;
            }

            let trend_result = self.calculate_trend(var_name, series)?;
            results.push(trend_result);
        }

        Ok(results)
    }

    /// Calculate trend for a single time series
    fn calculate_trend(&self, variable: &str, series: &[f64]) -> Result<TrendResult> {
        // Simple linear regression for trend detection
        let n = series.len() as f64;
        let x_sum: f64 = (0..series.len()).map(|i| i as f64).sum();
        let y_sum: f64 = series.iter().sum();
        let xy_sum: f64 = series.iter().enumerate().map(|(i, &y)| i as f64 * y).sum();
        let x_sq_sum: f64 = (0..series.len()).map(|i| (i as f64).powi(2)).sum();

        let slope = (n * xy_sum - x_sum * y_sum) / (n * x_sq_sum - x_sum * x_sum);

        // Calculate R-squared for significance
        let y_mean = y_sum / n;
        let ss_res: f64 = series
            .iter()
            .enumerate()
            .map(|(i, &y)| {
                let predicted = slope * i as f64 + (y_mean - slope * x_sum / n);
                (y - predicted).powi(2)
            })
            .sum();
        let ss_tot: f64 = series.iter().map(|&y| (y - y_mean).powi(2)).sum();
        let r_squared = 1.0 - (ss_res / ss_tot);

        let direction = if slope.abs() < 0.001 {
            TrendDirection::Stationary
        } else if slope > 0.0 {
            TrendDirection::Increasing
        } else {
            TrendDirection::Decreasing
        };

        // Simple significance test based on R-squared and sample size
        // Require at least 3 points for regression and RÂ² > 0.7 for strong trends
        let significant = r_squared > 0.7 && n >= 3.0;

        Ok(TrendResult {
            variable: variable.to_string(),
            direction,
            strength: r_squared.min(1.0),
            significant,
        })
    }

    /// Validate input data
    fn validate_data(&self, data: &HashMap<String, Vec<f64>>) -> Result<()> {
        if data.is_empty() {
            return Err(anyhow!("No data provided for analysis"));
        }

        let first_len = data.values().next().unwrap().len();
        if first_len > self.config.max_data_points {
            warn!(
                "Data size {} exceeds maximum {}, truncating",
                first_len, self.config.max_data_points
            );
        }

        for (var, series) in data {
            if series.is_empty() {
                return Err(anyhow!("Variable '{}' has no data points", var));
            }
            if !series.iter().all(|&x| x.is_finite()) {
                return Err(anyhow!("Variable '{}' contains non-finite values", var));
            }
        }

        Ok(())
    }

    /// Estimate memory usage for analysis
    fn estimate_memory_usage(&self, data: &HashMap<String, Vec<f64>>) -> usize {
        let total_points: usize = data.values().map(|v| v.len()).sum();
        // Rough estimate: 8 bytes per f64 + overhead
        total_points * 8 + data.len() * 100
    }

    /// Cumulative distribution function for t-distribution (simplified approximation)
    fn t_cdf(t: f64, df: f64) -> f64 {
        // Simplified approximation using normal CDF for large df
        if df > 30.0 {
            Self::normal_cdf(t)
        } else {
            // More accurate approximation for small df
            let a = 0.5
                * (1.0
                    + t / (df + t * t).sqrt() * Self::beta_inc(0.5 * df, 0.5, df / (df + t * t)));
            a.clamp(0.0, 1.0)
        }
    }

    /// Normal cumulative distribution function
    fn normal_cdf(x: f64) -> f64 {
        0.5 * (1.0 + Self::erf(x / 2.0_f64.sqrt()))
    }

    /// Error function approximation
    fn erf(x: f64) -> f64 {
        let a1 = 0.254829592;
        let a2 = -0.284496736;
        let a3 = 1.421413741;
        let a4 = -1.453152027;
        let a5 = 1.061405429;
        let p = 0.3275911;

        let sign = if x < 0.0 { -1.0 } else { 1.0 };
        let x = x.abs();

        let t = 1.0 / (1.0 + p * x);
        let y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * (-x * x).exp();

        sign * y
    }

    /// Incomplete beta function (simplified)
    fn beta_inc(_a: f64, _b: f64, _x: f64) -> f64 {
        // Simplified implementation - in practice, you'd use a proper library
        0.5 // Placeholder
    }
}

/// Changepoint detection wrapper
#[derive(Debug)]
pub struct ChangepointDetector {
    engine: StatisticalEngine,
}

impl ChangepointDetector {
    pub fn new() -> Result<Self> {
        Ok(Self {
            engine: StatisticalEngine::new()?,
        })
    }

    pub fn detect(&mut self, data: &HashMap<String, Vec<f64>>) -> Result<Vec<ChangepointResult>> {
        let results = self.engine.analyze_time_series(data)?;
        Ok(results.changepoints)
    }
}

/// Correlation analysis wrapper
#[derive(Debug)]
pub struct CorrelationAnalyzer {
    engine: StatisticalEngine,
}

impl CorrelationAnalyzer {
    pub fn new() -> Result<Self> {
        Ok(Self {
            engine: StatisticalEngine::new()?,
        })
    }

    pub fn analyze(&mut self, data: &HashMap<String, Vec<f64>>) -> Result<Vec<CorrelationResult>> {
        let results = self.engine.analyze_time_series(data)?;
        Ok(results.correlations)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;

    #[test]
    fn test_statistical_engine_creation() {
        let engine = StatisticalEngine::new();
        assert!(engine.is_ok());
    }

    #[ignore]
    #[test]
    fn test_correlation_calculation() -> Result<()> {
        let mut engine = StatisticalEngine::new()?;
        let mut data = HashMap::new();
        data.insert("x".to_string(), vec![1.0, 2.0, 3.0, 4.0, 5.0]);
        data.insert("y".to_string(), vec![2.0, 4.0, 6.0, 8.0, 10.0]);

        let results = engine.analyze_time_series(&data)?;
        assert!(!results.correlations.is_empty());

        let corr = &results.correlations[0];
        assert_eq!(corr.variables, ("x".to_string(), "y".to_string()));
        assert!((corr.coefficient - 1.0).abs() < 0.01);
        assert!(corr.significant);

        Ok(())
    }

    #[test]
    fn test_trend_analysis() -> Result<()> {
        let mut engine = StatisticalEngine::new()?;
        let mut data = HashMap::new();
        data.insert("trend".to_string(), vec![1.0, 2.0, 3.0, 4.0, 5.0]);

        let results = engine.analyze_time_series(&data)?;
        assert!(!results.trends.is_empty());

        let trend = &results.trends[0];
        assert_eq!(trend.variable, "trend");
        assert!(matches!(trend.direction, TrendDirection::Increasing));
        assert!(trend.significant);

        Ok(())
    }

    #[test]
    fn test_data_validation() {
        let engine = StatisticalEngine::new().unwrap();
        let mut data = HashMap::new();

        // Empty data should fail
        assert!(engine.validate_data(&data).is_err());

        // Data with NaN should fail
        data.insert("bad".to_string(), vec![1.0, f64::NAN, 3.0]);
        assert!(engine.validate_data(&data).is_err());
    }

    // BOCD Implementation Tests
    #[test]
    fn test_bocpd_state_creation() {
        let config = BOCPDConfig::default();
        let bocpd = StreamingBOCPD::new(config);
        
        assert_eq!(bocpd.short_term_state.processed_points, 0);
        assert_eq!(bocpd.medium_term_state.processed_points, 0);
        assert_eq!(bocpd.long_term_state.processed_points, 0);
        assert_eq!(bocpd.short_term_state.data_buffer.len(), 0);
    }

    #[test]
    fn test_joint_anomaly_changepoint_detection() {
        let config = BOCPDConfig {
            hazard_rate: 100.0,
            expected_run_length: 50,
            max_run_length_hypotheses: 200,
            alert_threshold: 0.8,
            multi_resolution: true,
            short_window: 25,
            medium_window: 50,
            long_window: 100,
        };
        
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Create data with a clear changepoint at index 25
        let mut data = Vec::new();
        for i in 0..20 {
            data.push(10.0 + (i as f64 * 0.1)); // Gradually increasing
        }
        for i in 20..40 {
            data.push(20.0 + (i as f64 * 0.2)); // Clear shift to higher values
        }
        
        let results = bocpd.detect_changepoints(&data).unwrap();
        
        // Should detect at least one changepoint
        assert!(!results.is_empty());
        
        // At least one result should have high confidence
        let high_confidence_results: Vec<_> = results
            .iter()
            .filter(|r| r.confidence > 0.5)
            .collect();
        assert!(!high_confidence_results.is_empty());
    }

    #[test]
    fn test_posterior_distribution_computation() {
        let config = BOCPDConfig::default();
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Test with simple data
        let test_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 10.0, 11.0, 12.0]; // Clear break at index 5
        
        for &value in &test_data {
            let short_state = bocpd.update_bocpd_state(value).unwrap();
            
            // Check that posterior is properly normalized (sum should be close to 1)
            let normalized = bocpd.normalize_distribution(&short_state.log_posterior);
            let sum: f64 = normalized.iter().sum();
            assert!((sum - 1.0).abs() < 1e-10, "Posterior should be normalized, got sum: {}", sum);
        }
    }

    #[test]
    fn test_streaming_updates_and_circular_buffers() {
        let config = BOCPDConfig {
            max_run_length_hypotheses: 100,
            short_window: 5,
            ..Default::default()
        };
        
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Add data that exceeds buffer size
        for i in 0..10 {
            let state = bocpd.update_bocpd_state(i as f64).unwrap();
            assert_eq!(state.data_buffer.len(), (i + 1).min(5));
        }
        
        // Verify buffer size is maintained
        assert_eq!(bocpd.short_term_state.data_buffer.len(), 5);
        
        // Verify oldest values are removed
        let buffer_values: Vec<f64> = bocpd.short_term_state.data_buffer.iter().cloned().collect();
        assert_eq!(buffer_values, vec![5.0, 6.0, 7.0, 8.0, 9.0]);
    }

    #[test]
    fn test_hazard_rate_adaptation() {
        let config = BOCPDConfig {
            hazard_rate: 200.0,
            ..Default::default()
        };
        
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Add data with low variance first
        for i in 0..15 {
            bocpd.update_bocpd_state(10.0 + (i as f64 * 0.01)).unwrap();
        }
        
        let initial_hazard = bocpd.short_term_state.hazard_rate;
        
        // Add data with high variance
        for i in 0..15 {
            let value = 10.0 + (i as f64 * 10.0); // Much higher variance
            bocpd.update_bocpd_state(value).unwrap();
        }
        
        let adapted_hazard = bocpd.short_term_state.hazard_rate;
        
        // Hazard rate should adapt (either increase or decrease based on variance)
        assert_ne!(initial_hazard, adapted_hazard);
    }

    #[test]
    fn test_multi_resolution_detection() {
        let config = BOCPDConfig {
            multi_resolution: true,
            short_window: 10,
            medium_window: 20,
            long_window: 30,
            ..Default::default()
        };
        
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Create data with multiple types of patterns
        let data = vec![
            // Short-term pattern: gentle oscillation
            1.0, 1.1, 1.0, 1.1, 1.0, 1.1, 1.0, 1.1, 1.0, 1.1,
            // Medium-term shift: mean change
            5.0, 5.1, 5.0, 5.1, 5.0, 5.1, 5.0, 5.1, 5.0, 5.1,
            // Long-term trend: clear trend change
            10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0,
        ];
        
        let results = bocpd.detect_changepoints(&data).unwrap();
        
        // Should detect patterns at different resolutions
        let resolutions: Vec<_> = results.iter().map(|r| &r.resolution_level).collect();
        
        // May detect patterns at different levels (depending on data characteristics)
        assert!(!resolutions.is_empty());
    }

    #[test]
    fn test_edge_cases() {
        let config = BOCPDConfig::default();
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Test empty data
        let empty_results = bocpd.detect_changepoints(&[]);
        assert!(empty_results.is_ok());
        assert!(empty_results.unwrap().is_empty());
        
        // Test constant series (no changepoints expected)
        let constant_data = vec![5.0; 30];
        let constant_results = bocpd.detect_changepoints(&constant_data).unwrap();
        // Should not detect many changepoints in constant data
        let high_confidence_count = constant_results.iter().filter(|r| r.confidence > 0.8).count();
        assert!(high_confidence_count <= 2, "Constant series should not have many high-confidence changepoints");
        
        // Test rapid changes (multiple changepoints)
        let rapid_changes = vec![
            1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 2.0, 2.0, 2.0, 15.0, 15.0, 15.0, 3.0, 3.0, 3.0
        ];
        let rapid_results = bocpd.detect_changepoints(&rapid_changes).unwrap();
        
        // Should detect multiple changepoints in rapidly changing data
        assert!(rapid_results.len() >= 2, "Rapidly changing data should have multiple changepoints");
    }

    #[test]
    fn test_numerical_stability() {
        let config = BOCPDConfig::default();
        let mut bocpd = StreamingBOCPD::new(config);
        
        // Test with extreme values
        let extreme_data = vec![
            1e10, 1e10, -1e10, 1e-10, 1e-10, f64::MAX, f64::MIN_POSITIVE
        ];
        
        let results = bocpd.detect_changepoints(&extreme_data);
        assert!(results.is_ok(), "Should handle extreme values gracefully");
        
        // Test log-space arithmetic functions
        let test_values = vec![-1000.0, -500.0, -100.0, 0.0, 100.0, 500.0, 1000.0];
        let log_sum = super::log_sum_exp(&test_values);
        assert!(log_sum.is_finite(), "Log-sum-exp should be finite");
        
        let log_add = super::log_add_exp(-1000.0, -500.0);
        assert!(log_add.is_finite(), "Log-add-exp should be finite");
    }
}
